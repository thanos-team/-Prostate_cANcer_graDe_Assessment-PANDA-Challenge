{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames[:2]:\n        print(os.path.join(dirname, filename))","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/prostate-cancer-grade-assessment/sample_submission.csv\n/kaggle/input/prostate-cancer-grade-assessment/test.csv\n/kaggle/input/prostate-cancer-grade-assessment/train_images/2673584f9398ce0acb21a86a1a711088.tiff\n/kaggle/input/prostate-cancer-grade-assessment/train_images/5d46da93924a4b15472581eb39658309.tiff\n/kaggle/input/prostate-cancer-grade-assessment/train_label_masks/9bce8bb47c22ab502ed7266e2e3762c0_mask.tiff\n/kaggle/input/prostate-cancer-grade-assessment/train_label_masks/124a0616099409f5b9aaedfd3ac3ab6d_mask.tiff\n/kaggle/input/nhan-hdf5/img_dtb1.h5\n/kaggle/input/nhan-hdf5/img_dtb2.h5\n/kaggle/input/h5-files/full_data_coordinate.h5\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\n# There are two ways to load the data from the PANDA dataset:\n# Option 1: Load images using openslide\nimport openslide\n# Option 2: Load images using skimage (requires that tifffile is installed)\nimport skimage.io\nimport random\nimport seaborn as sns\nimport cv2\n\n# General packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport PIL\nfrom IPython.display import Image, display\n\n# Plotly for the interactive viewer (see last section)\nimport plotly.graph_objs as go\n\n# Location of the training images\n\nBASE_PATH = '../input/prostate-cancer-grade-assessment'\n\n# image and mask directories\ndata_dir = f'{BASE_PATH}/train_images'\nmask_dir = f'{BASE_PATH}/train_label_masks'\n\n\n# Location of training labels\ntrain = pd.read_csv(f'{BASE_PATH}/train.csv').set_index('image_id')\ntest = pd.read_csv(f'{BASE_PATH}/test.csv')\n\ntrain_labels = pd.read_csv('/kaggle/input/prostate-cancer-grade-assessment/train.csv').set_index('image_id')\n\nsubmission = pd.read_csv(f'{BASE_PATH}/sample_submission.csv')\n\ndata_dirname = [name.replace('.tiff', '') for name in os.listdir(data_dir)]\nid_dir = [name.replace('_mask.tiff', '') for name in os.listdir(mask_dir)]\nid_dir.sort()\nid_dir.remove('3790f55cad63053e956fb73027179707')\nimg_id_mutual_name = id_dir\nimg_id_mutual_name[:5], len(img_id_mutual_name)","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"(['0005f7aaab2800f6170c399693a96917',\n  '000920ad0b612851f8e01bcc880d9b3d',\n  '0018ae58b01bdadc8e347995b69f99aa',\n  '001c62abd11fa4b57bf7a6c603a11bb9',\n  '001d865e65ef5d2579c190a0e0350d8f'],\n 10515)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tables\nimport h5py\nimport deepdish as dd\nimport tables\nimport h5py\nfrom sys import getsizeof\nimport deepdish as dd","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import deepdish as dd\n\ndf = dd.io.load('/kaggle/input/h5-files/full_data_coordinate.h5')\nlen(df)//36, len(df[0]), df[0]","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"(9790, 5, ['0005f7aaab2800f6170c399693a96917', 13312, 13824, 7168, 7680])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data_and_mask(ID, coordinates, level = 1):\n    \"\"\"\n    \n    \"\"\"\n    data_img = skimage.io.MultiImage(os.path.join(data_dir, f'{ID}.tiff'))[level]\n    mask_img = skimage.io.MultiImage(os.path.join(mask_dir, f'{ID}_mask.tiff'))[level]\n    coordinates = [coordinate // 2**(2*level) for coordinate in coordinates]\n    data_tile = data_img[coordinates[0]: coordinates[1], coordinates[2]: coordinates[3], :]\n    mask_tile = mask_img[coordinates[0]: coordinates[1], coordinates[2]: coordinates[3], :]\n    data_tile = cv2.resize(data_tile, (512, 512))\n    mask_tile = cv2.resize(mask_tile, (512, 512))\n    del data_img, mask_img\n    \n    # Load and return small image\n    return data_tile, mask_tile\n\n%time a, b = load_data_and_mask(df[0][0], df[0][1:], 1)\na.shape, b.shape","execution_count":5,"outputs":[{"output_type":"stream","text":"CPU times: user 354 ms, sys: 150 ms, total: 504 ms\nWall time: 564 ms\n","name":"stdout"},{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"((512, 512, 3), (512, 512, 3))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torch\n\nclass PANDADataset(Dataset):\n    def __init__(self, df, level=1, transform=None):\n        self.df = df\n        self.level = level\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index, level = 1):\n        ID = self.df[index][0]\n        coordinate = self.df[index][1: ]\n        image, mask = load_data_and_mask(ID, coordinate, level)\n        \n        return torch.tensor(image).permute(2, 0, 1), torch.tensor(mask).permute(2, 0, 1)[0]\n    \ncls = PANDADataset(df[:800], 1)\n%time cls[0][0].size(), cls[0][1].size(), len(cls)","execution_count":6,"outputs":[{"output_type":"stream","text":"CPU times: user 676 ms, sys: 260 ms, total: 936 ms\nWall time: 952 ms\n","name":"stdout"},{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"(torch.Size([3, 512, 512]), torch.Size([512, 512]), 800)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataLoader = DataLoader(cls, batch_size=4, shuffle=True, num_workers=4)\n\nfor i_batch, data in enumerate(dataLoader):\n    inputs, labels = data\n    print(i_batch, ': \\t', inputs.size(), ', ', labels.size())\n    del inputs, labels\n    if i_batch == 4:\n        break","execution_count":7,"outputs":[{"output_type":"stream","text":"0 : \t torch.Size([4, 3, 512, 512]) ,  torch.Size([4, 512, 512])\n1 : \t torch.Size([4, 3, 512, 512]) ,  torch.Size([4, 512, 512])\n2 : \t torch.Size([4, 3, 512, 512]) ,  torch.Size([4, 512, 512])\n3 : \t torch.Size([4, 3, 512, 512]) ,  torch.Size([4, 512, 512])\n4 : \t torch.Size([4, 3, 512, 512]) ,  torch.Size([4, 512, 512])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adapted from https://discuss.pytorch.org/t/unet-implementation/426\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels=1, n_classes=2, depth=5, wf=6, padding=False,\n                 batch_norm=False, up_mode='upconv'):\n        \"\"\"\n        Implementation of\n        U-Net: Convolutional Networks for Biomedical Image Segmentation\n        (Ronneberger et al., 2015)\n        https://arxiv.org/abs/1505.04597\n        Using the default arguments will yield the exact version used\n        in the original paper\n        Args:\n            in_channels (int): number of input channels\n            n_classes (int): number of output channels\n            depth (int): depth of the network\n            wf (int): number of filters in the first layer is 2**wf\n            padding (bool): if True, apply padding such that the input shape\n                            is the same as the output.\n                            This may introduce artifacts\n            batch_norm (bool): Use BatchNorm after layers with an\n                               activation function\n            up_mode (str): one of 'upconv' or 'upsample'.\n                           'upconv' will use transposed convolutions for\n                           learned upsampling.\n                           'upsample' will use bilinear upsampling.\n        \"\"\"\n        super(UNet, self).__init__()\n        assert up_mode in ('upconv', 'upsample')\n        self.padding = padding\n        self.depth = depth\n        prev_channels = in_channels\n        self.down_path = nn.ModuleList()\n        for i in range(depth):\n            self.down_path.append(UNetConvBlock(prev_channels, 2**(wf+i),\n                                                padding, batch_norm))\n            prev_channels = 2**(wf+i)\n\n        self.up_path = nn.ModuleList()\n        for i in reversed(range(depth - 1)):\n            self.up_path.append(UNetUpBlock(prev_channels, 2**(wf+i), up_mode,\n                                            padding, batch_norm))\n            prev_channels = 2**(wf+i)\n\n        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n\n    def forward(self, x):\n        blocks = []\n        for i, down in enumerate(self.down_path):\n            x = down(x)\n            if i != len(self.down_path)-1:\n                blocks.append(x)\n                x = F.avg_pool2d(x, 2)\n\n        for i, up in enumerate(self.up_path):\n            x = up(x, blocks[-i-1])\n\n        return self.last(x)\n\n\nclass UNetConvBlock(nn.Module):\n    def __init__(self, in_size, out_size, padding, batch_norm):\n        super(UNetConvBlock, self).__init__()\n        block = []\n\n        block.append(nn.Conv2d(in_size, out_size, kernel_size=3,\n                               padding=int(padding)))\n        block.append(nn.ReLU())\n        if batch_norm:\n            block.append(nn.BatchNorm2d(out_size))\n\n        block.append(nn.Conv2d(out_size, out_size, kernel_size=3,\n                               padding=int(padding)))\n        block.append(nn.ReLU())\n        if batch_norm:\n            block.append(nn.BatchNorm2d(out_size))\n\n        self.block = nn.Sequential(*block)\n\n    def forward(self, x):\n        out = self.block(x)\n        return out\n\n\nclass UNetUpBlock(nn.Module):\n    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n        super(UNetUpBlock, self).__init__()\n        if up_mode == 'upconv':\n            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2,\n                                         stride=2)\n        elif up_mode == 'upsample':\n            self.up = nn.Sequential(nn.Upsample(mode='bilinear', scale_factor=2),\n                                    nn.Conv2d(in_size, out_size, kernel_size=1))\n\n        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n\n    def center_crop(self, layer, target_size):\n        _, _, layer_height, layer_width = layer.size()\n        diff_y = (layer_height - target_size[0]) // 2\n        diff_x = (layer_width - target_size[1]) // 2\n        return layer[:, :, diff_y:(diff_y + target_size[0]), diff_x:(diff_x + target_size[1])]\n\n    def forward(self, x, bridge):\n        up = self.up(x)\n        crop1 = self.center_crop(bridge, up.shape[2:])\n        out = torch.cat([up, crop1], 1)\n        out = self.conv_block(out)\n\n        return out","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --- unet params\n#these parameters get fed directly into the UNET class, and more description of them can be discovered there\n\nn_classes= 6    #number of classes in the data mask that we'll aim to predict\n\n\nin_channels= 3  #input channel of the data, RGB = 3\npadding= True   #should levels be padded\ndepth= 5       #depth of the network \nwf= 2           #wf (int): number of filters in the first layer is 2**wf, was 6\nup_mode= 'upconv' #should we simply upsample the mask, or should we try and learn an interpolation \nbatch_norm = True #should we use batch normalization between the layers\n\n# --- training params\nbatch_size=4\npatch_size=512\nnum_epochs = 10 # 100\n\nedge_weight = 1.1 #edges tend to be the most poorly segmented given how little area they occupy in the training set, this paramter boosts their values along the lines of the original UNET paper\nphases = [\"train\",\"val\"] #how many phases did we create databases for?\nvalidation_phases= [\"val\"] #when should we do valiation? note that validation is time consuming, so as opposed to doing for both training and validation, we do it only for vlaidation at the end of the epoch","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpuid=0\n#specify if we should use a GPU (cuda) or only the CPU\nif(torch.cuda.is_available()):\n    print(torch.cuda.get_device_properties(gpuid))\n    torch.cuda.set_device(gpuid)\n    device = torch.device(f'cuda:{gpuid}')\nelse:\n    device = torch.device(f'cpu')\n","execution_count":10,"outputs":[{"output_type":"stream","text":"_CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build the model according to the paramters specified above and copy it to the GPU. finally print out the number of trainable parameters\nmodel = UNet(n_classes=n_classes, in_channels=in_channels, padding=padding,depth=depth,wf=wf, up_mode=up_mode, batch_norm=batch_norm).to(device)\nprint(f\"total params: \\t{sum([np.prod(p.size()) for p in model.parameters()])}\")","execution_count":11,"outputs":[{"output_type":"stream","text":"total params: \t122486\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"optim = torch.optim.Adam(model.parameters()) #adam is going to be the most robust\ncriterion = nn.CrossEntropyLoss(reduce=False)","execution_count":12,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n  warnings.warn(warning.format(ret))\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(num_epochs):\n    #model.train()  # Set model to training mode\n    running_loss = 0.0\n    \n    for i, data in enumerate(dataLoader, 0):\n        inputs, labels = data\n        inputs = inputs.to(device,dtype=torch.float) \n        labels = labels.type('torch.LongTensor').to(device)\n        ##with torch.set_grad_enabled(True):\n        # zero the parameter gradients\n        optim.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        loss.sum().backward()\n        optim.step()\n        # print statistics\n        \n        running_loss += loss.mean()\n        if i % 200 == 199:    # print every 200 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 200))\n            running_loss = 0.0\n\nprint('Finished Training')","execution_count":13,"outputs":[{"output_type":"stream","text":"[1,   200] loss: 1.569\n[2,   200] loss: 1.324\n[3,   200] loss: 1.157\n[4,   200] loss: 1.095\n[5,   200] loss: 1.058\n[6,   200] loss: 1.048\n[7,   200] loss: 1.031\n[8,   200] loss: 1.025\n[9,   200] loss: 1.018\n[10,   200] loss: 1.012\nFinished Training\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}