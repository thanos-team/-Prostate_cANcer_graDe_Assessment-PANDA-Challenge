{"cells":[{"metadata":{},"cell_type":"markdown","source":"### 0. Basic libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport tables\nimport h5py","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Creating HDF5 files\n\n#### 1.1. create & store an `array_vars`\n\nFirstly, mock up some simple dummy data to save to our file."},{"metadata":{"trusted":true},"cell_type":"code","source":"d1 = np.random.random(size = (1000,20))\nd2 = np.random.random(size = (1000,200))","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The **first step to creating a HDF5 file is to initialise it**. \n\nIt uses a very similar syntax to initialising a typical text file in numpy. The first argument provides the filename and location, the second the mode. We’re writing the file, so we provide a w for write access."},{"metadata":{"trusted":true},"cell_type":"code","source":"hf = h5py.File('data.h5', 'w')","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This creates a file object, `hf`, which has a bunch of associated methods. One is `create_dataset`, which does what it says on the tin. Just provide a name for the dataset, and the numpy array."},{"metadata":{"trusted":true},"cell_type":"code","source":"hf.create_dataset('vars1', data=d1)\nhf.create_dataset('vars2', data=d2)","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"<HDF5 dataset \"vars2\": shape (1000, 200), type \"<f8\">"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"All we need to do now is close the file, which will write all of our work to disk."},{"metadata":{"trusted":true},"cell_type":"code","source":"hf.close()","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1.2. create & store a dictionary"},{"metadata":{"trusted":true},"cell_type":"code","source":"d = {'x_vrs': [[1,2], [1, 0]], 'y_vrs': 2.5}","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import deepdish as dd\nimport numpy as np\n\nX = np.zeros((2, 2, 2, 2))\ny = np.arange(1, 3, 1)\n\ndd.io.save('test.h5', {'data': X, 'label': y}, compression=None)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1.3. create & store a `string_vars`"},{"metadata":{"trusted":true},"cell_type":"code","source":"h5 = h5py.File(\"model_text1\",\"w\")\nh5[\"test\"] = \"Hello World\"\nh5[\"train\"] = 'go'","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A weak_point / disapointage of this method is we can not store a list of string directly."},{"metadata":{"trusted":true},"cell_type":"code","source":"try : \n    h5.create_dataset('val', data = ['a', 'b'])\nexcept TypeError as err:\n    print(\"TypeErrors: \", err)","execution_count":9,"outputs":[{"output_type":"stream","text":"TypeErrors:  No conversion path for dtype: dtype('<U1')\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"h5.close()","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we will use `deepdish (dd)` again to load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"text1 = ['hello', 'fuck', 'press a button']\ntext2 = ['never', 'say', 'neverous']\ndd.io.save('model_text2.h5', [text1, text2])","execution_count":11,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/deepdish/io/hdf5io.py:251: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n  elif _pandas and isinstance(level, (pd.DataFrame, pd.Series, pd.Panel)):\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"### 2. Reading HDF5 files.\n\n#### 2.1. Using data saved by `h5py` (here is h5py.File('data.h5', 'w'))\n\nTo `open` and `read` data, we use the same File method in `read mode`: `r`."},{"metadata":{"trusted":true},"cell_type":"code","source":"hf = h5py.File('data.h5', 'r')\nhf","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"<HDF5 file \"data.h5\" (mode r)>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"To see what data is in this file, we can call the keys() method on the file object."},{"metadata":{"trusted":true},"cell_type":"code","source":"hf.keys()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"<KeysViewHDF5 ['vars1', 'vars2']>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":" **Grab** each dataset we created above using the `get method`, specifying the `name`."},{"metadata":{"trusted":true},"cell_type":"code","source":"name1 = hf.get('vars1')\nname1","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"<HDF5 dataset \"vars1\": shape (1000, 20), type \"<f8\">"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"This returns a `HDF5` dataset object. To convert this to an array, just call numpy’s array method."},{"metadata":{"trusted":true},"cell_type":"code","source":"name1 = np.array(name1)\nname1.shape","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"(1000, 20)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"hf.close()","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.2. Saving by using `dd` (here is `dd.io.save('model_text.h5', text)`) is equivalent with using `h5py` but faster"},{"metadata":{"trusted":true},"cell_type":"code","source":"hf = h5py.File('test.h5', 'r')","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hf.keys()","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"<KeysViewHDF5 ['data', 'label']>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"name1 = hf.get('data')\nname2 = hf.get('label')\n\nname1, name2","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"(<HDF5 dataset \"data\": shape (2, 2, 2, 2), type \"<f8\">,\n <HDF5 dataset \"label\": shape (2,), type \"<i8\">)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"name1.shape, name2.shape","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"((2, 2, 2, 2), (2,))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"name2[:]","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"array([1, 2])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"name1[0]","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"array([[[0., 0.],\n        [0., 0.]],\n\n       [[0., 0.],\n        [0., 0.]]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"hf.close()","execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Beside that, we can call the insight_variables by using `deep_dish`"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = dd.io.load('test.h5')\na","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"{'data': array([[[[0., 0.],\n          [0., 0.]],\n \n         [[0., 0.],\n          [0., 0.]]],\n \n \n        [[[0., 0.],\n          [0., 0.]],\n \n         [[0., 0.],\n          [0., 0.]]]]),\n 'label': array([1, 2])}"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### 2.3. Likewise, now we using `h5py` to loading `model_text2.h5`"},{"metadata":{"trusted":true},"cell_type":"code","source":"hf = h5py.File('model_text2.h5', 'r')\nhf.keys()","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"<KeysViewHDF5 ['data']>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"name = hf.get('data')\nname = str(name)","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"name","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"'<HDF5 group \"/data\" (2 members)>'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"hf.close()","execution_count":28,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But this method is not useful to call the `insight_variable` then using `deepdish`"},{"metadata":{"trusted":true},"cell_type":"code","source":"Z = dd.io.load('model_text2.h5')\nprint(Z[0], '\\n', Z[1])","execution_count":29,"outputs":[{"output_type":"stream","text":"['hello', 'fuck', 'press a button'] \n ['never', 'say', 'neverous']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### 3. Groups.\n\nGroups are the basic container mechanism in a HDF5 file, allowing hierarchical organisation of the data. Groups are created similarly to datasets, and datsets are then added using the group object."},{"metadata":{"trusted":true},"cell_type":"code","source":"d0 = np.random.random(size = (100))\nd1 = np.random.random(size = (100,33))\nd2 = np.random.random(size = (100,333))\nd3 = np.random.random(size = (100,3333))","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hf = h5py.File('data2.h5', 'w')","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g1 = hf.create_group('group1')","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g1.create_dataset('data1',data=d1)\ng1.create_dataset('data2',data=d1)","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"<HDF5 dataset \"data2\": shape (100, 33), type \"<f8\">"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We can also create subfolders. Just specify the group name as a directory format."},{"metadata":{"trusted":true},"cell_type":"code","source":"g2 = hf.create_group('group2/subfolder2')","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g2.create_dataset('data3',data=d3)","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"<HDF5 dataset \"data3\": shape (100, 3333), type \"<f8\">"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"As before, to read data in irectories and subdirectories use the `get method` with the full `subdirectory path`."},{"metadata":{"trusted":true},"cell_type":"code","source":"group2 = hf.get('group2/subfolder2')\ngroup2.items()","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"ItemsViewHDF5(<HDF5 group \"/group2/subfolder2\" (1 members)>)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"group1 = hf.get('group1')\ngroup1.items()","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"ItemsViewHDF5(<HDF5 group \"/group1\" (2 members)>)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"n1 = group1.get('data1')\nnp.array(n1).shape","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"(100, 33)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"hf.close()","execution_count":39,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Compression\nTo save on disk space, while sacrificing read speed, you can compress the data. Just add the compression argument, which can be either gzip, lzf or szip. gzip is the most portable, as it’s available with every HDF5 install, lzf is the fastest but doesn’t compress as effectively as gzip, and szip is a NASA format that is patented up; if you don’t know about it, chances are your organisation doesn’t have the patent, so avoid.\n\nFor gzip you can also specify the additional compression_opts argument, which sets the compression level. The default is 4, but it can be an integer between 0 and 9."},{"metadata":{"trusted":true},"cell_type":"code","source":"hf = h5py.File('data.h5', 'w')\n\nhf.create_dataset('dataset_1', data=d1, compression=\"gzip\", compression_opts=9)\nhf.create_dataset('dataset_2', data=d2, compression=\"gzip\", compression_opts=9)\n\nhf.close()","execution_count":40,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Examples:\nCreate `database` which contains 3 columns: 2 first columns is dictionaries of the 3D_arrays (for instance size 256x256x3); the last one is a list_string of names\n\n#### 5.1. Step1. Create datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"vars1 = dict({'img_data': np.random.randint(0, 255, size = (10, 256, 256, 3)), 'idx': 10})\nvars1['img_data'].shape, vars1['idx']","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"((10, 256, 256, 3), 10)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Noting that we can not use `vars1.img_data` to call the `insight_variables`; because"},{"metadata":{"trusted":true},"cell_type":"code","source":"try: vars1.img_data\nexcept AttributeError as err:\n    print('AttributeError:', err)","execution_count":42,"outputs":[{"output_type":"stream","text":"AttributeError: 'dict' object has no attribute 'img_data'\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Going on `vars2` & `vars3`"},{"metadata":{"trusted":true},"cell_type":"code","source":"vars2 = dict({'img_data': np.random.randint(0, 5, size = (10, 256, 256, 3)), 'idx': 10})\nvars3 = ['abc0001.tiff']","execution_count":43,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.2. Step2. Storing vars to `file.hfd5`"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = {'data_image': vars1, 'mask_image': vars2, 'image_id': vars3}","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dd.io.save('image_database.h5', data = data)","execution_count":45,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.2.1. Loading a string to `hdf5`"},{"metadata":{"trusted":true},"cell_type":"code","source":"A = dd.io.load('image_database.h5')\ntype(A), len(A)","execution_count":46,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"(dict, 3)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(A['data_image']), len(A['data_image']), type(A['mask_image']), len(A['mask_image'])","execution_count":47,"outputs":[{"output_type":"execute_result","execution_count":47,"data":{"text/plain":"(dict, 2, dict, 2)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(A['data_image']['img_data']), len(A['data_image']['img_data'])","execution_count":48,"outputs":[{"output_type":"execute_result","execution_count":48,"data":{"text/plain":"(numpy.ndarray, 10)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(A['mask_image']['img_data']), len(A['mask_image']['img_data'])","execution_count":49,"outputs":[{"output_type":"execute_result","execution_count":49,"data":{"text/plain":"(numpy.ndarray, 10)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### 6. Loading to get a full file"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport openslide\nimport skimage\nimport skimage.io\nimport random\nimport seaborn as sns\nimport cv2\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport PIL\nfrom IPython.display import Image, display\nimport plotly.graph_objs as go","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BASE_PATH = '../input/prostate-cancer-grade-assessment'\n\n# image and mask directories\ndata_dir = f'{BASE_PATH}/train_images'\nmask_dir = f'{BASE_PATH}/train_label_masks'\n\n\n# Location of training labels\ntrain = pd.read_csv(f'{BASE_PATH}/train.csv').set_index('image_id')\ntest = pd.read_csv(f'{BASE_PATH}/test.csv')\n\ntrain_labels = pd.read_csv('/kaggle/input/prostate-cancer-grade-assessment/train.csv').set_index('image_id')\n\nsubmission = pd.read_csv(f'{BASE_PATH}/sample_submission.csv')","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = [ '07a7ef0ba3bb0d6564a73f4f3e1c2293',\n            '037504061b9fba71ef6e24c48c6df44d',\n            '035b1edd3d1aeeffc77ce5d248a01a53',\n            '059cbf902c5e42972587c8d17d49efed',\n            '06a0cbd8fd6320ef1aa6f19342af2e68',\n            '06eda4a6faca84e84a781fee2d5f47e1',\n            '0a4b7a7499ed55c71033cefb0765e93d',\n            '0838c82917cd9af681df249264d2769c',\n            '046b35ae95374bfb48cdca8d7c83233f',\n            '074c3e01525681a275a42282cd21cbde',\n            '05abe25c883d508ecc15b6e857e59f32',\n            '05f4e9415af9fdabc19109c980daf5ad',\n            '060121a06476ef401d8a21d6567dee6d',\n            '068b0e3be4c35ea983f77accf8351cc8',\n            '08f055372c7b8a7e1df97c6586542ac8']\n","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_tiles(img_id, level, mode=0, n_tiles = 81, ops = 256):\n        \"\"\"\n            Input: \n                    - img_id (str): image_id from the train dataset\n                    - level (int): an integer in {0, 1, 2} corresponding to the level_downsamples {1, 4, 16}\n                    - mode (int) : define the quantities of pad_height & pad_width\n                    - n_tiles (int): number of tiles (must be a squared_number)\n                    - ops (int) : output_size of each image\n            return: \n                    - list of img_data_tiles\n                    - img_mask\n                    - bool\n        \"\"\"\n        tile_size = int(256 / 2**(2*level))\n        data_img = skimage.io.MultiImage(os.path.join(data_dir, f'{img_id}.tiff'))[level]\n        mask_img = skimage.io.MultiImage(os.path.join(mask_dir, f'{img_id}_mask.tiff'))[level]\n        \n        image_data_ls = []; image_mask_ls = []\n        \n        h, w = data_img.shape[:2]\n        pad_h = (tile_size - h % tile_size) % tile_size + ((tile_size * mode) // 2)\n        pad_w = (tile_size - w % tile_size) % tile_size + ((tile_size * mode) // 2)\n\n        img2_dt_ = np.pad(data_img,[[pad_h // 2, pad_h - pad_h // 2], [pad_w // 2,pad_w - pad_w//2], [0,0]], constant_values = 255)\n        img2_ms_ = np.pad(mask_img,[[pad_h // 2, pad_h - pad_h // 2], [pad_w // 2,pad_w - pad_w//2], [0,0]], constant_values = mask_img.max())\n        \n        img3_dt_ = img2_dt_.reshape(img2_dt_.shape[0] // tile_size, tile_size,\n                                    img2_dt_.shape[1] // tile_size, tile_size,\n                                    3 )\n        img3_ms_ = img2_ms_.reshape(img2_ms_.shape[0] // tile_size, tile_size,\n                                    img2_ms_.shape[1] // tile_size, tile_size,\n                                    3 )\n        \n        img3_dt_ = img3_dt_.transpose(0,2,1,3,4).reshape(-1, tile_size, tile_size,3)\n        img3_ms_ = img3_ms_.transpose(0,2,1,3,4).reshape(-1, tile_size, tile_size,3)\n        \n        n_tiles_with_info = (img3_dt_.reshape(img3_dt_.shape[0],-1).sum(1) < tile_size ** 2 * 3 * 255).sum()\n        \n        if len(img) < n_tiles:\n            img3_dt_ = np.pad(img3_dt_,[[0,N - len(img3_dt_)],[0,0],[0,0],[0,0]], constant_values=255)\n            img3_ms_ = np.pad(img3_ms_,[[0,N - len(img3_ms_)],[0,0],[0,0],[0,0]], constant_values = mask_img.max())\n            \n        idxs_dt_ = np.argsort(img3_dt_.reshape(img3_dt_.shape[0],-1).sum(-1))[:n_tiles]    \n        \n        img3_dt_ = img3_dt_[idxs_dt_]\n        img3_ms_ = img3_ms_[idxs_dt_]\n        \n        for i in range(len(img3_dt_)):\n            img4_dt_ = cv2.resize(img3_dt_[i], (ops, ops))\n            image_data_ls.append({'img':img4_dt_, 'idx':i})\n            img4_ms_ = cv2.resize(img3_ms_[i], (ops, ops))\n            image_mask_ls.append({'img':img4_ms_, 'idx':i})\n        \n        return image_data_ls, image_mask_ls, n_tiles_with_info >= n_tiles","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dataloader(list_images, level, mode=0, n_tiles = 81, ops = 256):\n    \n    vars1 = []\n    vars2 = []\n    vrs_bool = []\n    vars3 = list_images\n    for img_id in list_images:\n        vrs1, vrs2, bools = get_tiles(img_id, level, mode=0, n_tiles = 81, ops = 256)\n        vars1.append(vrs1)\n        vars2.append(vrs2)\n        vrs_bool.append(bools)\n    data = {'data_image': vars1, 'mask_image': vars2, 'image_id': vars3, 'bools_val' : vrs_bool}\n    return data","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = dataloader(images, 1)\ndd.io.save('image_database.h5', data = data)","execution_count":56,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}